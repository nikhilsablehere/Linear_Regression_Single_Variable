{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e709b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c575b3e",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "\n",
    "We will use a simple data set with only two data points - a house with 1000 square feet(sqft) sold for \\\\$300,000 and a house with 2000 square feet sold for \\\\$500,000. These two points will constitute our *data or training set*. The units of size are 1000 sqft and the units of price are 1000s of dollars.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| -------------------| ------------------------ |\n",
    "| 1.0               | 300                      |\n",
    "| 2.0               | 500                      |\n",
    "\n",
    "You would like to fit a linear regression model through these two points, so you can then predict price for other houses - say, a house with 1200 sqft and automate the process of optimizing $w$ and $b$ using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f53b0",
   "metadata": {},
   "source": [
    "### Setting up input & target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c84b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train is the input variable (size in 1000s of square feet)\n",
    "# y_train is the target (price in 1000s of dollars)\n",
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0, 500.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ac109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n",
      "[300. 500.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339eb1c9",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d017d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    cost_sum = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        cost_sum = cost_sum + cost\n",
    "    total_cost = (1 / (2 * m)) * cost_sum\n",
    "    \n",
    "    return total_cost    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab47681",
   "metadata": {},
   "source": [
    "## Gradient descent summary\n",
    "So far, we have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, we utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training we measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f76fc",
   "metadata": {},
   "source": [
    "\n",
    "*gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaneously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b9f0f",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent\n",
    "We will implement gradient descent algorithm for one feature. You will need three functions. \n",
    "- `compute_gradient` implementing equation (4) and (5) above\n",
    "- `compute_cost` implementing equation (2) above (code from previous lab)\n",
    "- `gradient_descent`, utilizing compute_gradient and compute_cost\n",
    "\n",
    "Conventions:\n",
    "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
    "- w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8704173",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.3\"></a>\n",
    "### Compute Gradient\n",
    "<a name='ex-01'></a>\n",
    "`compute_gradient`  implements (4) and (5) above and returns $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. The embedded comments describe the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad9170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = f_wb - y[i]\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "        \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00c9e3",
   "metadata": {},
   "source": [
    "###  Compute Gradient Descent\n",
    "Now that gradients can be computed,  gradient descent, described in equation (3) above can be implemented below in `gradient_descent`. The details of the implementation are described in the comments. Below, you will utilize this function to find optimal values of $w$ and $b$ on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32e6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in, b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "        \n",
    "        # Update Parameters using equation (3) above\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Save the J at each iteration\n",
    "        if i < 100000: # Prevent resource exhaustion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "            \n",
    "        # Print cost every at intervals 10 times as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e} \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "            \n",
    "    return w, b, J_history, p_history # Return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ac446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 7.93e+04  dj_dw: -6.500e+02, dj_db: -4.000e+02  w:  6.500e+00, b: 4.00000e+00\n",
      "Iteration 1000: Cost 3.41e+00  dj_dw: -3.712e-01, dj_db:  6.007e-01  w:  1.949e+02, b: 1.08228e+02\n",
      "Iteration 2000: Cost 7.93e-01  dj_dw: -1.789e-01, dj_db:  2.895e-01  w:  1.975e+02, b: 1.03966e+02\n",
      "Iteration 3000: Cost 1.84e-01  dj_dw: -8.625e-02, dj_db:  1.396e-01  w:  1.988e+02, b: 1.01912e+02\n",
      "Iteration 4000: Cost 4.28e-02  dj_dw: -4.158e-02, dj_db:  6.727e-02  w:  1.994e+02, b: 1.00922e+02\n",
      "Iteration 5000: Cost 9.95e-03  dj_dw: -2.004e-02, dj_db:  3.243e-02  w:  1.997e+02, b: 1.00444e+02\n",
      "Iteration 6000: Cost 2.31e-03  dj_dw: -9.660e-03, dj_db:  1.563e-02  w:  1.999e+02, b: 1.00214e+02\n",
      "Iteration 7000: Cost 5.37e-04  dj_dw: -4.657e-03, dj_db:  7.535e-03  w:  1.999e+02, b: 1.00103e+02\n",
      "Iteration 8000: Cost 1.25e-04  dj_dw: -2.245e-03, dj_db:  3.632e-03  w:  2.000e+02, b: 1.00050e+02\n",
      "Iteration 9000: Cost 2.90e-05  dj_dw: -1.082e-03, dj_db:  1.751e-03  w:  2.000e+02, b: 1.00024e+02\n",
      "(w,b) found by gradient descent: (199.9929,100.0116)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddc751",
   "metadata": {},
   "source": [
    "Take a moment and note some characteristics of the gradient descent process printed above.  \n",
    "\n",
    "- The cost starts large and rapidly declines.\n",
    "- The partial derivatives, `dj_dw`, and `dj_db` also get smaller, rapidly at first and then more slowly.\n",
    "- progress slows though the learning rate, alpha, remains fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df136d5",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "Now that we have discovered the optimal values for the parameters $w$ and $b$, we can now use the model to predict housing values based on our learned parameters. As expected, the predicted values are nearly the same as the training values for the same housing. Further, the value not in the prediction is in line with the expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4320c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 sqft house prediction 300.0 Thousand dollars\n",
      "1200 sqft house prediction 340.0 Thousand dollars\n",
      "2000 sqft house prediction 500.0 Thousand dollars\n"
     ]
    }
   ],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ab43c",
   "metadata": {},
   "source": [
    "### Increased Learning Rate\n",
    "In the lecture, there was a discussion related to the proper value of the learning rate, $\\alpha$ in equation(3). The larger $\\alpha$ is, the faster gradient descent will converge to a solution. But, if it is too large, gradient descent will diverge. Above you have an example of a solution which converges nicely.\n",
    "\n",
    "Let's try increasing the value of  $\\alpha$ and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31fa3887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 2.58e+05  dj_dw: -6.500e+02, dj_db: -4.000e+02  w:  5.200e+02, b: 3.20000e+02\n",
      "Iteration    1: Cost 7.82e+05  dj_dw:  1.130e+03, dj_db:  7.000e+02  w: -3.840e+02, b:-2.40000e+02\n",
      "Iteration    2: Cost 2.37e+06  dj_dw: -1.970e+03, dj_db: -1.216e+03  w:  1.192e+03, b: 7.32800e+02\n",
      "Iteration    3: Cost 7.19e+06  dj_dw:  3.429e+03, dj_db:  2.121e+03  w: -1.551e+03, b:-9.63840e+02\n",
      "Iteration    4: Cost 2.18e+07  dj_dw: -5.974e+03, dj_db: -3.691e+03  w:  3.228e+03, b: 1.98886e+03\n",
      "Iteration    5: Cost 6.62e+07  dj_dw:  1.040e+04, dj_db:  6.431e+03  w: -5.095e+03, b:-3.15579e+03\n",
      "Iteration    6: Cost 2.01e+08  dj_dw: -1.812e+04, dj_db: -1.120e+04  w:  9.402e+03, b: 5.80237e+03\n",
      "Iteration    7: Cost 6.09e+08  dj_dw:  3.156e+04, dj_db:  1.950e+04  w: -1.584e+04, b:-9.80139e+03\n",
      "Iteration    8: Cost 1.85e+09  dj_dw: -5.496e+04, dj_db: -3.397e+04  w:  2.813e+04, b: 1.73730e+04\n",
      "Iteration    9: Cost 5.60e+09  dj_dw:  9.572e+04, dj_db:  5.916e+04  w: -4.845e+04, b:-2.99567e+04\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# set alpha to a large value\n",
    "iterations = 10\n",
    "tmp_alpha = 8.0e-1\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66355317",
   "metadata": {},
   "source": [
    "Above, $w$ and $b$ are bouncing back and forth between positive and negative with the absolute value increasing with each iteration. Further, each iteration $\\frac{\\partial J(w,b)}{\\partial w}$ changes sign and cost is increasing rather than decreasing. This is a clear sign that the *learning rate is too large* and the solution is diverging. \n",
    "Let's visualize this with a plot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
